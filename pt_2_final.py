# -*- coding: utf-8 -*-
"""Pt 2 final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ZH6gla_S8Z7KEzqdlKv0nfgwuzFUNnS
"""

s
import os, time, numpy as np, tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE

# Eager mode
tf.compat.v1.enable_eager_execution()
assert tf.executing_eagerly(), "Eager execution must be enabled."

def name_to_seed(name: str) -> int:
    import string
    acc = 0
    for ch in name.strip():
        if ch in string.ascii_letters + string.digits:
            acc = (acc * 37 + ord(ch)) % (2**31 - 1)
    return acc if acc > 0 else 1234

NAME_FOR_SEED = "Rohith"
SEED = name_to_seed(NAME_FOR_SEED)
np.random.seed(SEED); tf.random.set_seed(SEED)

# Global dataset
IMG_H, IMG_W = 28, 28
IMG_SIZE_FLAT = IMG_H * IMG_W
N_CLASSES = 10
IMG_SHAPE = (IMG_H, IMG_W)



from tensorflow.keras.datasets import fashion_mnist

def load_split(split_at=50_000):
    (x_tr, y_tr), (x_te, y_te) = fashion_mnist.load_data()
    x_tr = (x_tr.astype(np.float32)/255.0).reshape(-1, IMG_SIZE_FLAT)
    x_te = (x_te.astype(np.float32)/255.0).reshape(-1, IMG_SIZE_FLAT)
    x_val, y_val = x_tr[split_at:], y_tr[split_at:]
    x_tr, y_tr   = x_tr[:split_at], y_tr[:split_at]
    y_tr_1h = tf.one_hot(y_tr, depth=N_CLASSES, dtype=tf.float32).numpy()
    y_val_1h = tf.one_hot(y_val, depth=N_CLASSES, dtype=tf.float32).numpy()
    y_te_1h  = tf.one_hot(y_te,  depth=N_CLASSES, dtype=tf.float32).numpy()
    return (x_tr, y_tr, y_tr_1h), (x_val, y_val, y_val_1h), (x_te, y_te, y_te_1h)


# Model / training utilities

def make_optimizer(name:str, lr:float):
    name = name.lower()
    if name == "adam":
        return tf.compat.v1.train.AdamOptimizer(lr)
    if name == "momentum":
        return tf.compat.v1.train.MomentumOptimizer(lr, momentum=0.9)
    if name == "sgd":
        return tf.compat.v1.train.GradientDescentOptimizer(lr)
    raise ValueError("optimizer must be 'adam' | 'momentum' | 'sgd'")

def run_experiment(optimizer_name="adam", lr=1e-3, batch_size=256, n_epochs=15,
                   split_at=50_000, device=None, l2=0.0, plot_everything=False):

    # Data
    (x_tr, y_tr, y_tr_1h), (x_val, y_val, y_val_1h), (x_te, y_te, y_te_1h) = load_split(split_at)
    ds_tr  = tf.data.Dataset.from_tensor_slices((x_tr, y_tr_1h)).shuffle(4*batch_size, seed=SEED).batch(batch_size)
    ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val_1h)).batch(batch_size)
    ds_te  = tf.data.Dataset.from_tensor_slices((x_te,  y_te_1h )).batch(batch_size)

    # Variables
    w = tf.Variable(tf.random.truncated_normal([IMG_SIZE_FLAT, N_CLASSES], stddev=0.01, dtype=tf.float32, seed=SEED))
    b = tf.Variable(tf.zeros([N_CLASSES], dtype=tf.float32))

    # Forward
    def logits_fn(x): return tf.matmul(x, w) + b
    def loss_fn(y1h, logits):
        ce = tf.nn.softmax_cross_entropy_with_logits(labels=y1h, logits=logits)
        reg = l2 * tf.nn.l2_loss(w) if l2>0 else 0.0
        return tf.reduce_mean(ce) + reg
    def batch_correct(y1h, logits):
        pred = tf.nn.softmax(logits)
        return tf.reduce_sum(tf.cast(tf.equal(tf.argmax(pred,1), tf.argmax(y1h,1)), tf.float32))

    opt = make_optimizer(optimizer_name, lr)

    device_ctx = "/device:CPU:0"
    if device == "GPU" and tf.config.list_physical_devices("GPU"): device_ctx = "/device:GPU:0"
    elif device in (None, "AUTO") and tf.config.list_physical_devices("GPU"): device_ctx = "/device:GPU:0"

    hist = {"train_acc":[], "val_acc":[], "train_loss":[], "val_loss":[], "epoch_time":[]}

    with tf.device(device_ctx):
        for epoch in range(1, n_epochs+1):
            t0 = time.time()
            # train
            seen=0; loss_sum=0.0; cor=0.0
            for xb,yb in ds_tr:
                with tf.GradientTape() as tape:
                    lg = logits_fn(xb)
                    ls = loss_fn(yb, lg)
                gw, gb = tape.gradient(ls, [w,b])
                opt.apply_gradients(zip([gw,gb],[w,b]))
                loss_sum += float(ls.numpy())*xb.shape[0]
                cor      += float(batch_correct(yb, lg).numpy())
                seen     += int(xb.shape[0])
            tr_loss = loss_sum/seen; tr_acc = cor/seen

            # val
            seen=0; loss_sum=0.0; cor=0.0
            for xb,yb in ds_val:
                lg = logits_fn(xb)
                ls = loss_fn(yb, lg)
                loss_sum += float(ls.numpy())*xb.shape[0]
                cor      += float(batch_correct(yb, lg).numpy())
                seen     += int(xb.shape[0])
            va_loss = loss_sum/seen; va_acc = cor/seen

            hist["train_acc"].append(tr_acc);  hist["val_acc"].append(va_acc)
            hist["train_loss"].append(tr_loss);hist["val_loss"].append(va_loss)
            hist["epoch_time"].append(time.time()-t0)

            print(f"Epoch {epoch:02d}: train_loss={tr_loss:.4f} acc={tr_acc:.4f} | val_loss={va_loss:.4f} acc={va_acc:.4f}")

    # test
    seen=0; cor=0.0
    for xb,yb in ds_te:
        lg = logits_fn(xb)
        cor += float(batch_correct(yb, lg).numpy())
        seen += int(xb.shape[0])
    te_acc = cor/seen

    print(f"\nTest acc ({optimizer_name}, bs={batch_size}, split={split_at}, l2={l2}): {te_acc:.4f}")
    print(f"Mean epoch time on {device_ctx}: {np.mean(hist['epoch_time']):.3f}s (median {np.median(hist['epoch_time']):.3f}s)")

    # plots
    if plot_everything:
        epochs = np.arange(1, len(hist["train_acc"])+1)

        # Accuracy
        plt.figure()
        plt.plot(epochs, hist["train_acc"], label="Train")
        plt.plot(epochs, hist["val_acc"], label="Val")
        plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.title("Train vs Val Accuracy")
        plt.legend(); plt.tight_layout(); plt.show()

        # Loss
        plt.figure()
        plt.plot(epochs, hist["train_loss"], label="Train")
        plt.plot(epochs, hist["val_loss"], label="Val")
        plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Train vs Val Loss")
        plt.legend(); plt.tight_layout(); plt.show()

        # 3x3 predictions grid
        sample_idx  = np.arange(9)
        (x_tr_all, y_tr_all), (x_te_all, y_te_all) = fashion_mnist.load_data()[0], fashion_mnist.load_data()[1]
        x_test_flat = (x_te_all.astype(np.float32)/255.0).reshape(-1, IMG_SIZE_FLAT)
        imgs = x_test_flat[sample_idx]
        true = y_te_all[sample_idx]
        lg   = logits_fn(tf.convert_to_tensor(imgs, dtype=tf.float32))
        pred = np.argmax(tf.nn.softmax(lg).numpy(), axis=1)

        fig, axes = plt.subplots(3,3)
        fig.subplots_adjust(hspace=0.3, wspace=0.3)
        for i, ax in enumerate(axes.flat):
            ax.imshow(imgs[i].reshape(IMG_SHAPE), cmap='binary')
            ax.set_xlabel(f"True: {true[i]}, Pred: {pred[i]}")
            ax.set_xticks([]); ax.set_yticks([])
        plt.show()

        # Weight heatmaps
        Wv = w.numpy()
        w_min, w_max = float(np.min(Wv)), float(np.max(Wv))
        fig, axes = plt.subplots(3,4)
        fig.subplots_adjust(hspace=0.3, wspace=0.3)
        for i, ax in enumerate(axes.flat):
            if i < N_CLASSES:
                ax.imshow(Wv[:,i].reshape(IMG_SHAPE), vmin=w_min, vmax=w_max, cmap='seismic')
                ax.set_xlabel(f"Weights: {i}")
            ax.set_xticks([]); ax.set_yticks([])
        plt.show()

        # Confusion matrix
        y_true_all=[]; y_pred_all=[]
        for xb,yb in ds_te:
            lg = logits_fn(xb)
            y_true_all.append(np.argmax(yb.numpy(), axis=1))
            y_pred_all.append(np.argmax(tf.nn.softmax(lg).numpy(), axis=1))
        y_true_all = np.concatenate(y_true_all); y_pred_all = np.concatenate(y_pred_all)
        cm = confusion_matrix(y_true_all, y_pred_all)
        plt.figure()
        plt.imshow(cm)
        plt.title("Confusion Matrix (TF Logistic)")
        plt.xlabel("Predicted"); plt.ylabel("True")
        for i in range(N_CLASSES):
            for j in range(N_CLASSES):
                plt.text(j, i, str(cm[i,j]), ha='center', va='center', fontsize=7)
        plt.tight_layout(); plt.show()

        # t-SNE of class weight vectors (+ K-Means colors)
        W_cls = w.numpy().T
        kmeans = KMeans(n_clusters=3, n_init=10, random_state=SEED)
        km_labels = kmeans.fit_predict(W_cls)
        tsne = TSNE(n_components=2, init='random', perplexity=3, learning_rate='auto', random_state=SEED)
        W_2d = tsne.fit_transform(W_cls)
        plt.figure()
        plt.scatter(W_2d[:,0], W_2d[:,1], s=120, c=km_labels, cmap="tab10", edgecolors='k')
        for i in range(N_CLASSES):
            plt.annotate(str(i), (W_2d[i,0], W_2d[i,1]), textcoords="offset points", xytext=(5,5))
        plt.title("t-SNE of class-weight vectors (colored by K-Means)")
        plt.xlabel("dim 1"); plt.ylabel("dim 2"); plt.tight_layout(); plt.show()

    return {
        "optimizer": optimizer_name, "batch_size": batch_size, "epochs": n_epochs,
        "split_at": split_at, "device": device or "AUTO",
        "mean_epoch_time": float(np.mean(hist["epoch_time"])),
        "test_acc": float(te_acc)
    }


print("\n=== MAIN RUN: Adam, bs=256, epochs=15, split 50k/10k (with plots) ===")
main_result = run_experiment(
    optimizer_name="adam", lr=1e-3, batch_size=256, n_epochs=15,
    split_at=50_000, device="AUTO", l2=0.0, plot_everything=True
)
print("Main result:", main_result)

print("\n=== OPTIMIZER COMPARISON (same settings, no plots) ===")
opt_sgd      = run_experiment("sgd",      lr=1e-2, batch_size=256, n_epochs=15, split_at=50_000, device="AUTO", l2=0.0)
opt_momentum = run_experiment("momentum", lr=1e-2, batch_size=256, n_epochs=15, split_at=50_000, device="AUTO", l2=0.0)
opt_adam     = run_experiment("adam",     lr=1e-3, batch_size=256, n_epochs=15, split_at=50_000, device="AUTO", l2=0.0)
print("SGD:     ", opt_sgd)
print("Momentum:", opt_momentum)
print("Adam:    ", opt_adam)

print("\n=== BATCH SIZE EFFECT (Adam) ===")
bs_64  = run_experiment("adam", lr=1e-3, batch_size=64,  n_epochs=15, split_at=50_000, device="AUTO", l2=0.0)
bs_512 = run_experiment("adam", lr=1e-3, batch_size=512, n_epochs=15, split_at=50_000, device="AUTO", l2=0.0)
print("Batch 64 :", bs_64)
print("Batch 512:", bs_512)

print("\n=== TRAIN/VAL SPLIT CHANGE (Adam, bs=256) ===")
split_48k = run_experiment("adam", lr=1e-3, batch_size=256, n_epochs=15, split_at=48_000, device="AUTO", l2=0.0)
print("Split 48k/12k:", split_48k)

print("\n=== CPU vs GPU MEAN EPOCH TIME (Adam, bs=256, 10 epochs) ===")
cpu_timing = run_experiment("adam", lr=1e-3, batch_size=256, n_epochs=10, split_at=50_000, device="CPU", l2=0.0)
gpu_timing = run_experiment("adam", lr=1e-3, batch_size=256, n_epochs=10, split_at=50_000, device="GPU", l2=0.0)
print({"CPU_mean_epoch": cpu_timing["mean_epoch_time"], "GPU_mean_epoch": gpu_timing["mean_epoch_time"]})

print("\n=== OVERFITTING MITIGATION (L2 weight decay) ===")
l2_run = run_experiment("adam", lr=1e-3, batch_size=256, n_epochs=15, split_at=50_000, device="AUTO", l2=1e-4)
print("With L2=1e-4:", l2_run)


print("\n=== scikit-learn BASELINES (RF & LinearSVC) ===")
# Reload data once
(x_tr, y_tr, _), (x_val, y_val, _), (x_te, y_te, _) = load_split(50_000)

rf = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=SEED)
t0 = time.time(); rf.fit(x_tr, y_tr); rf_time = time.time()-t0
rf_train = accuracy_score(y_tr, rf.predict(x_tr))
rf_val   = accuracy_score(y_val, rf.predict(x_val))
rf_test  = accuracy_score(y_te, rf.predict(x_te))
print(f"RandomForest: train={rf_train:.4f} val={rf_val:.4f} test={rf_test:.4f} | fit_time={rf_time:.2f}s")

svm = LinearSVC(C=1.0, max_iter=5000, dual=True, random_state=SEED)
t0 = time.time(); svm.fit(x_tr, y_tr); svm_time = time.time()-t0
svm_train = accuracy_score(y_tr, svm.predict(x_tr))
svm_val   = accuracy_score(y_val, svm.predict(x_val))
svm_test  = accuracy_score(y_te, svm.predict(x_te))
print(f"LinearSVC:    train={svm_train:.4f} val={svm_val:.4f} test={svm_test:.4f} | fit_time={svm_time:.2f}s")